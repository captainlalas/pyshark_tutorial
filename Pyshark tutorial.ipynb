{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737c1480",
   "metadata": {},
   "source": [
    "## Welcome to our Pyspark tutorial\n",
    "\n",
    "### 1. Intalling pyspark and reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91519145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alex</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>120000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iannis</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Pro Tennis</td>\n",
       "      <td>9345000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>1400000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald</td>\n",
       "      <td>30.0</td>\n",
       "      <td>IT</td>\n",
       "      <td>85000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cedric</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Marketing Manager</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mathew</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>78000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aboubakar</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Designer</td>\n",
       "      <td>65000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dan</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>98000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chris</td>\n",
       "      <td>28.0</td>\n",
       "      <td>IT</td>\n",
       "      <td>83000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Steven</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>88000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rob</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>97000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name   Age         Profession     Income\n",
       "0        Alex  25.0     Data Scientist   120000.0\n",
       "1      Iannis  22.0         Pro Tennis  9345000.0\n",
       "2     William  26.0       Entrepreneur  1400000.0\n",
       "3      Donald  30.0                 IT    85000.0\n",
       "4      Cedric  31.0  Marketing Manager    90000.0\n",
       "5      Mathew  27.0         Accountant    78000.0\n",
       "6   Aboubakar  34.0                NaN        NaN\n",
       "7         NaN   NaN           Designer    65000.0\n",
       "8         Dan  29.0     Data Scientist    98000.0\n",
       "9       Chris  28.0                 IT    83000.0\n",
       "10     Steven  31.0         Accountant    88000.0\n",
       "11        Rob  38.0       Entrepreneur    97000.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After installing pyspark we will import it and start playing with\n",
    "# Please install pyspark first if it is not yet done with the command \"pip install pyspark\"\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"name.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50028d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb09fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a4b3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.13:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1caf9dd9c40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eaeabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3a0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset with the first row as header\n",
    "df_pyspark = spark.read.option('header','true').csv('name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a955d76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- Income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_pyspark.head(4)\n",
    "# And now we can check the data types\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc0a97",
   "metadata": {},
   "source": [
    "### 2. Pyspark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7cc3699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's read the dataset with the appropriate data type for each columns\n",
    "df_pyspark = spark.read.csv('name.csv', header=True, inferSchema=True)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4816385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Profession', 'Income']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the columns\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2effd972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|     Name| Age|\n",
      "+---------+----+\n",
      "|     Alex|  25|\n",
      "|   Iannis|  22|\n",
      "|  William|  26|\n",
      "|   Donald|  30|\n",
      "|   Cedric|  31|\n",
      "|   Mathew|  27|\n",
      "|Aboubakar|  34|\n",
      "|     NULL|NULL|\n",
      "|      Dan|  29|\n",
      "|    Chris|  28|\n",
      "|   Steven|  31|\n",
      "|      Rob|  38|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to select a specific column or a list of columns with the \"select\" operation\n",
    "df_pyspark.select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06cd964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+----------+------------------+\n",
      "|summary|     Name|               Age|Profession|            Income|\n",
      "+-------+---------+------------------+----------+------------------+\n",
      "|  count|       11|                11|        11|                11|\n",
      "|   mean|     NULL|29.181818181818183|      NULL|1049909.0909090908|\n",
      "| stddev|     NULL| 4.400413203738526|      NULL|2779160.0333390464|\n",
      "|    min|Aboubakar|                22|Accountant|             65000|\n",
      "|    max|  William|                38|Pro Tennis|           9345000|\n",
      "+-------+---------+------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the data types and describe summary\n",
    "df_pyspark.dtypes\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7f2e998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------------+-------+----------+\n",
      "|     Name| Age|       Profession| Income|Experience|\n",
      "+---------+----+-----------------+-------+----------+\n",
      "|     Alex|  25|   Data Scientist| 120000|         4|\n",
      "|   Iannis|  22|       Pro Tennis|9345000|         5|\n",
      "|  William|  26|     Entrepreneur|1400000|         2|\n",
      "|   Donald|  30|               IT|  85000|         3|\n",
      "|   Cedric|  31|Marketing Manager|  90000|         4|\n",
      "|   Mathew|  27|       Accountant|  78000|         3|\n",
      "|Aboubakar|  34|             NULL|   NULL|      NULL|\n",
      "|     NULL|NULL|         Designer|  65000|      NULL|\n",
      "|      Dan|  29|   Data Scientist|  98000|      NULL|\n",
      "|    Chris|  28|               IT|  83000|      NULL|\n",
      "|   Steven|  31|       Accountant|  88000|      NULL|\n",
      "|      Rob|  38|     Entrepreneur|  97000|      NULL|\n",
      "+---------+----+-----------------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Adding a new column in dataframe, could be also done by aggregating an existing col\n",
    "from pyspark.sql.functions import when, lit\n",
    "exp_list = [4, 5, 2, 6, 4, 3]\n",
    "new_df = df_pyspark.withColumn(\"Experience\",\n",
    "                              when((df_pyspark.Name == \"Alex\") | (df_pyspark.Name == \"Cedric\"), lit(4)).\n",
    "                               when((df_pyspark.Name == \"Iannis\"), lit(5)).\n",
    "                               when((df_pyspark.Name == \"William\"), lit(2)).\n",
    "                               when((df_pyspark.Name == \"Donald\") | (df_pyspark.Name == \"Mathew\"), lit(3)))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea7f99",
   "metadata": {},
   "source": [
    "### 3. Handling Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a84558e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------+\n",
      "| Age|       Profession| Income|\n",
      "+----+-----------------+-------+\n",
      "|  25|   Data Scientist| 120000|\n",
      "|  22|       Pro Tennis|9345000|\n",
      "|  26|     Entrepreneur|1400000|\n",
      "|  30|               IT|  85000|\n",
      "|  31|Marketing Manager|  90000|\n",
      "|  27|       Accountant|  78000|\n",
      "|  34|             NULL|   NULL|\n",
      "|NULL|         Designer|  65000|\n",
      "|  29|   Data Scientist|  98000|\n",
      "|  28|               IT|  83000|\n",
      "|  31|       Accountant|  88000|\n",
      "|  38|     Entrepreneur|  97000|\n",
      "+----+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dropping columns\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f387838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------------+-------+\n",
      "|   Name|Age|       Profession| Income|\n",
      "+-------+---+-----------------+-------+\n",
      "|   Alex| 25|   Data Scientist| 120000|\n",
      "| Iannis| 22|       Pro Tennis|9345000|\n",
      "|William| 26|     Entrepreneur|1400000|\n",
      "| Donald| 30|               IT|  85000|\n",
      "| Cedric| 31|Marketing Manager|  90000|\n",
      "| Mathew| 27|       Accountant|  78000|\n",
      "|    Dan| 29|   Data Scientist|  98000|\n",
      "|  Chris| 28|               IT|  83000|\n",
      "| Steven| 31|       Accountant|  88000|\n",
      "|    Rob| 38|     Entrepreneur|  97000|\n",
      "+-------+---+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now let's focus on dropping a rows with null values with na\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb5656ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------------+-------+\n",
      "|     Name| Age|       Profession| Income|\n",
      "+---------+----+-----------------+-------+\n",
      "|     Alex|  25|   Data Scientist| 120000|\n",
      "|   Iannis|  22|       Pro Tennis|9345000|\n",
      "|  William|  26|     Entrepreneur|1400000|\n",
      "|   Donald|  30|               IT|  85000|\n",
      "|   Cedric|  31|Marketing Manager|  90000|\n",
      "|   Mathew|  27|       Accountant|  78000|\n",
      "|Aboubakar|  34|             NULL|   NULL|\n",
      "|     NULL|NULL|         Designer|  65000|\n",
      "|      Dan|  29|   Data Scientist|  98000|\n",
      "|    Chris|  28|               IT|  83000|\n",
      "|   Steven|  31|       Accountant|  88000|\n",
      "|      Rob|  38|     Entrepreneur|  97000|\n",
      "+---------+----+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Dropping with how=any is the default and similar to the previous line of code, let's try any=all\n",
    "df_pyspark.na.drop(how=\"all\").show() #We notice that nothing happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a09be32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------------+-------+\n",
      "|     Name| Age|       Profession| Income|\n",
      "+---------+----+-----------------+-------+\n",
      "|     Alex|  25|   Data Scientist| 120000|\n",
      "|   Iannis|  22|       Pro Tennis|9345000|\n",
      "|  William|  26|     Entrepreneur|1400000|\n",
      "|   Donald|  30|               IT|  85000|\n",
      "|   Cedric|  31|Marketing Manager|  90000|\n",
      "|   Mathew|  27|       Accountant|  78000|\n",
      "|Aboubakar|  34|             NULL|   NULL|\n",
      "|     NULL|NULL|         Designer|  65000|\n",
      "|      Dan|  29|   Data Scientist|  98000|\n",
      "|    Chris|  28|               IT|  83000|\n",
      "|   Steven|  31|       Accountant|  88000|\n",
      "|      Rob|  38|     Entrepreneur|  97000|\n",
      "+---------+----+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Threshold is the min number of missing values to drop the row\n",
    "df_pyspark.na.drop(how=\"any\", thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a275e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------------+-------+\n",
      "|   Name| Age|       Profession| Income|\n",
      "+-------+----+-----------------+-------+\n",
      "|   Alex|  25|   Data Scientist| 120000|\n",
      "| Iannis|  22|       Pro Tennis|9345000|\n",
      "|William|  26|     Entrepreneur|1400000|\n",
      "| Donald|  30|               IT|  85000|\n",
      "| Cedric|  31|Marketing Manager|  90000|\n",
      "| Mathew|  27|       Accountant|  78000|\n",
      "|   NULL|NULL|         Designer|  65000|\n",
      "|    Dan|  29|   Data Scientist|  98000|\n",
      "|  Chris|  28|               IT|  83000|\n",
      "| Steven|  31|       Accountant|  88000|\n",
      "|    Rob|  38|     Entrepreneur|  97000|\n",
      "+-------+----+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Subset of dropping\n",
    "df_pyspark.na.drop(how=\"any\", subset=['Income']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10df744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the missing values with mean of the column with Imputer, you can also set the strategy to median\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(inputCols=['Age', 'Income'],\n",
    "                 outputCols=[\"{}_imputed\".format(c) for c in ['Age', 'Income']]).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eab3e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------------+-------+-----------+--------------+\n",
      "|     Name| Age|       Profession| Income|Age_imputed|Income_imputed|\n",
      "+---------+----+-----------------+-------+-----------+--------------+\n",
      "|     Alex|  25|   Data Scientist| 120000|         25|        120000|\n",
      "|   Iannis|  22|       Pro Tennis|9345000|         22|       9345000|\n",
      "|  William|  26|     Entrepreneur|1400000|         26|       1400000|\n",
      "|   Donald|  30|               IT|  85000|         30|         85000|\n",
      "|   Cedric|  31|Marketing Manager|  90000|         31|         90000|\n",
      "|   Mathew|  27|       Accountant|  78000|         27|         78000|\n",
      "|Aboubakar|  34|             NULL|   NULL|         34|         90000|\n",
      "|     NULL|NULL|         Designer|  65000|         29|         65000|\n",
      "|      Dan|  29|   Data Scientist|  98000|         29|         98000|\n",
      "|    Chris|  28|               IT|  83000|         28|         83000|\n",
      "|   Steven|  31|       Accountant|  88000|         31|         88000|\n",
      "|      Rob|  38|     Entrepreneur|  97000|         38|         97000|\n",
      "+---------+----+-----------------+-------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d572a1d",
   "metadata": {},
   "source": [
    "### 4. Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad8df94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  Name| Age|Income|\n",
      "+------+----+------+\n",
      "|Donald|  30| 85000|\n",
      "|Cedric|  31| 90000|\n",
      "|Mathew|  27| 78000|\n",
      "|  NULL|NULL| 65000|\n",
      "| Chris|  28| 83000|\n",
      "|Steven|  31| 88000|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Income <= 90000 and selecting specific columns \n",
    "df_pyspark.filter(\"Income<=90000\").select(['Name', 'Age', 'Income']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29a72886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Profession|Income|\n",
      "+------+---+----------+------+\n",
      "|Donald| 30|        IT| 85000|\n",
      "|Mathew| 27|Accountant| 78000|\n",
      "| Chris| 28|        IT| 83000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple conditions with operator (&, | )\n",
    "df_pyspark.filter((df_pyspark['Income'] <=90000) & (df_pyspark['Age'] <= 30)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dab9a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------+-------+\n",
      "|   Name|Age|    Profession| Income|\n",
      "+-------+---+--------------+-------+\n",
      "|   Alex| 25|Data Scientist| 120000|\n",
      "| Iannis| 22|    Pro Tennis|9345000|\n",
      "|William| 26|  Entrepreneur|1400000|\n",
      "|    Dan| 29|Data Scientist|  98000|\n",
      "|    Rob| 38|  Entrepreneur|  97000|\n",
      "+-------+---+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Not Operator\n",
    "df_pyspark.filter(~(df_pyspark['Income'] <=90000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ab92d",
   "metadata": {},
   "source": [
    "### 5. GroupBy and Aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5ad8cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+-----------+\n",
      "|       Profession|sum(Age)|sum(Income)|\n",
      "+-----------------+--------+-----------+\n",
      "|Marketing Manager|      31|      90000|\n",
      "|             NULL|      34|       NULL|\n",
      "|         Designer|    NULL|      65000|\n",
      "|   Data Scientist|      54|     218000|\n",
      "|               IT|      58|     168000|\n",
      "|     Entrepreneur|      64|    1497000|\n",
      "|       Accountant|      58|     166000|\n",
      "|       Pro Tennis|      22|    9345000|\n",
      "+-----------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Groupby works with aggregate functions, let's try some!\n",
    "df_pyspark.groupBy('Profession').sum().show() \n",
    "#Aggregated both Age and Income because we didn't specificy a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77f02fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|       Profession|max(Income)|\n",
      "+-----------------+-----------+\n",
      "|Marketing Manager|      90000|\n",
      "|             NULL|       NULL|\n",
      "|         Designer|      65000|\n",
      "|   Data Scientist|     120000|\n",
      "|               IT|      85000|\n",
      "|     Entrepreneur|    1400000|\n",
      "|       Accountant|      88000|\n",
      "|       Pro Tennis|    9345000|\n",
      "+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's find out the max, mean salary by Profession\n",
    "df_pyspark.groupBy('Profession').max('Income').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d392f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|       Profession|count|\n",
      "+-----------------+-----+\n",
      "|Marketing Manager|    1|\n",
      "|             NULL|    1|\n",
      "|         Designer|    1|\n",
      "|   Data Scientist|    2|\n",
      "|               IT|    2|\n",
      "|     Entrepreneur|    2|\n",
      "|       Accountant|    2|\n",
      "|       Pro Tennis|    1|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of \n",
    "df_pyspark.groupBy('Profession').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c293f06",
   "metadata": {},
   "source": [
    "### 6. Simple ML Regression with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70915315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Income|\n",
      "+-------+---+----------+------+\n",
      "|   Alex| 25|         5|120000|\n",
      "| Iannis| 22|         4| 93500|\n",
      "|William| 26|         7|890000|\n",
      "| Donald| 30|        10| 85000|\n",
      "| Cedric| 31|         8| 90000|\n",
      "| Mathew| 27|         2| 78000|\n",
      "|  Tyler|  6|         6| 95000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading the new dataset into traning_df\n",
    "training_df = spark.read.csv('name_and_exp.csv', header=True, inferSchema=True)\n",
    "training_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2128cd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf6a06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let import the needed module to combine Age and Experience into our independent feature\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler = VectorAssembler(inputCols=[\"Age\",\"Experience\"], outputCol=\"Indep Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "949dc1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureassembler.transform(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f252314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+--------------+\n",
      "|   Name|Age|Experience|Income|Indep Features|\n",
      "+-------+---+----------+------+--------------+\n",
      "|   Alex| 25|         5|120000|    [25.0,5.0]|\n",
      "| Iannis| 22|         4| 93500|    [22.0,4.0]|\n",
      "|William| 26|         7|890000|    [26.0,7.0]|\n",
      "| Donald| 30|        10| 85000|   [30.0,10.0]|\n",
      "| Cedric| 31|         8| 90000|    [31.0,8.0]|\n",
      "| Mathew| 27|         2| 78000|    [27.0,2.0]|\n",
      "|  Tyler|  6|         6| 95000|     [6.0,6.0]|\n",
      "+-------+---+----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "835b375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data = output.select(\"Indep Features\", \"Income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d46eb936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Indep Features|Income|\n",
      "+--------------+------+\n",
      "|    [25.0,5.0]|120000|\n",
      "|    [22.0,4.0]| 93500|\n",
      "|    [26.0,7.0]|890000|\n",
      "|   [30.0,10.0]| 85000|\n",
      "|    [31.0,8.0]| 90000|\n",
      "|    [27.0,2.0]| 78000|\n",
      "|     [6.0,6.0]| 95000|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3348fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data, test_data = finalized_data.randomSplit([0.75, 0.25])\n",
    "regressor = LinearRegression(featuresCol='Indep Features', labelCol='Income')\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cae60552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-80274.5592, 115479.4291])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Coefficients of our regression model\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4104afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "pred_res = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cf73556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+------------------+\n",
      "|Indep Features|Income|        prediction|\n",
      "+--------------+------+------------------+\n",
      "|     [6.0,6.0]| 95000|1833027.2879932723|\n",
      "|    [27.0,2.0]| 78000|-314656.1712846339|\n",
      "+--------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_res.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f669b",
   "metadata": {},
   "source": [
    "End of the tutorial here, the following steps where completed in Databricks platform,\n",
    "[Here is the link of the notebook on Databricks](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5640477560811446/712977470269093/3978417781751019/latest.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550693b6",
   "metadata": {},
   "source": [
    "&copy; This notebook was inspired by the [freeCodeCamp.org Pyshark tutorial on YouTube](https://www.youtube.com/watch?v=_C8kWso4ne4). The original dataset was dropped, the one used was created by the owner of the notebook and some lines of code were intentionally changed for a better hands-on experience of the tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
