{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737c1480",
   "metadata": {},
   "source": [
    "## Welcome to our Pyspark tutorial\n",
    "\n",
    "### 1. Intalling pyspark and reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91519145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alex</td>\n",
       "      <td>25</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>120000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iannis</td>\n",
       "      <td>22</td>\n",
       "      <td>Pro Tennis</td>\n",
       "      <td>9345000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William</td>\n",
       "      <td>26</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>1400000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald</td>\n",
       "      <td>30</td>\n",
       "      <td>IT</td>\n",
       "      <td>85000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cedric</td>\n",
       "      <td>31</td>\n",
       "      <td>Marketing Manager</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mathew</td>\n",
       "      <td>27</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>78000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aboubakar</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>Designer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name  Age         Profession     Income\n",
       "0       Alex   25     Data Scientist   120000.0\n",
       "1     Iannis   22         Pro Tennis  9345000.0\n",
       "2    William   26       Entrepreneur  1400000.0\n",
       "3     Donald   30                 IT    85000.0\n",
       "4     Cedric   31  Marketing Manager    90000.0\n",
       "5     Mathew   27         Accountant    78000.0\n",
       "6  Aboubakar   34                NaN    65000.0\n",
       "7        NaN   19           Designer        NaN"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After installing pyspark we will import it and start playing with\n",
    "# Please install pyspark first if it is not yet done with the command \"pip install pyspark\"\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"name.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50028d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb09fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a4b3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.13:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x275c92b5f70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eaeabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db3a0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset with the first row as header\n",
    "df_pyspark = spark.read.option('header','true').csv('name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a955d76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- Income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_pyspark.head(4)\n",
    "# And now we can check the data types\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc0a97",
   "metadata": {},
   "source": [
    "### 2. Pyspark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7cc3699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's read the dataset with the appropriate data type for each columns\n",
    "df_pyspark = spark.read.csv('name.csv', header=True, inferSchema=True)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4816385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Profession', 'Income']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the columns\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2effd972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     Name|Age|\n",
      "+---------+---+\n",
      "|     Alex| 25|\n",
      "|   Iannis| 22|\n",
      "|  William| 26|\n",
      "|   Donald| 30|\n",
      "|   Cedric| 31|\n",
      "|   Mathew| 27|\n",
      "|Aboubakar| 34|\n",
      "|     NULL| 19|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to select a specific column or a list of columns with the \"select\" operation\n",
    "df_pyspark.select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06cd964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------------+----------+------------------+\n",
      "|summary|     Name|              Age|Profession|            Income|\n",
      "+-------+---------+-----------------+----------+------------------+\n",
      "|  count|        7|                8|         7|                 7|\n",
      "|   mean|     NULL|            26.75|      NULL|1597571.4285714286|\n",
      "| stddev|     NULL|4.891683905218266|      NULL| 3451169.311848901|\n",
      "|    min|Aboubakar|               19|Accountant|            120000|\n",
      "|    max|  William|               34|Pro Tennis|           9345000|\n",
      "+-------+---------+-----------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the data types and describe summary\n",
    "df_pyspark.dtypes\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7f2e998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------------+----------+----------+\n",
      "|   Name|Age|       Profession|    Income|Experience|\n",
      "+-------+---+-----------------+----------+----------+\n",
      "|   Alex| 25|   Data Scientist|  $120,000|         4|\n",
      "| Iannis| 22|       Pro Tennis|$9,345,000|         5|\n",
      "|William| 26|     Entrepreneur|$1,400,000|         2|\n",
      "| Donald| 30|               IT|   $85,000|         3|\n",
      "| Cedric| 31|Marketing Manager|   $90,000|         4|\n",
      "| Mathew| 27|       Accountant|   $78,000|         3|\n",
      "+-------+---+-----------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Adding a new column in dataframe, could be also done by aggregating an existing col\n",
    "from pyspark.sql.functions import when, lit\n",
    "exp_list = [4, 5, 2, 6, 4, 3]\n",
    "new_df = df_pyspark.withColumn(\"Experience\",\n",
    "                              when((df_pyspark.Name == \"Alex\") | (df_pyspark.Name == \"Cedric\"), lit(4)).\n",
    "                               when((df_pyspark.Name == \"Iannis\"), lit(5)).\n",
    "                               when((df_pyspark.Name == \"William\"), lit(2)).\n",
    "                               when((df_pyspark.Name == \"Donald\") | (df_pyspark.Name == \"Mathew\"), lit(3)))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea7f99",
   "metadata": {},
   "source": [
    "### 3. Handling Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a84558e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-----------+\n",
      "|Age|       Profession|     Income|\n",
      "+---+-----------------+-----------+\n",
      "| 25|   Data Scientist|  $120,000 |\n",
      "| 22|       Pro Tennis|$9,345,000 |\n",
      "| 26|     Entrepreneur|$1,400,000 |\n",
      "| 30|               IT|   $85,000 |\n",
      "| 31|Marketing Manager|   $90,000 |\n",
      "| 27|       Accountant|   $78,000 |\n",
      "| 34|             NULL|   $65,000 |\n",
      "| 19|         Designer|       NULL|\n",
      "+---+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dropping columns\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f387838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------------+-------+\n",
      "|   Name|Age|       Profession| Income|\n",
      "+-------+---+-----------------+-------+\n",
      "|   Alex| 25|   Data Scientist| 120000|\n",
      "| Iannis| 22|       Pro Tennis|9345000|\n",
      "|William| 26|     Entrepreneur|1400000|\n",
      "| Donald| 30|               IT|  85000|\n",
      "| Cedric| 31|Marketing Manager|  90000|\n",
      "| Mathew| 27|       Accountant|  78000|\n",
      "+-------+---+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now let's focus on dropping a rows with null values with na\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb5656ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------------+-----------+\n",
      "|   Name|Age|       Profession|     Income|\n",
      "+-------+---+-----------------+-----------+\n",
      "|   Alex| 25|   Data Scientist|  $120,000 |\n",
      "| Iannis| 22|       Pro Tennis|$9,345,000 |\n",
      "|William| 26|     Entrepreneur|$1,400,000 |\n",
      "| Donald| 30|               IT|   $85,000 |\n",
      "| Cedric| 31|Marketing Manager|   $90,000 |\n",
      "| Mathew| 27|       Accountant|   $78,000 |\n",
      "|   NULL| 34|             NULL|   $65,000 |\n",
      "|   NULL| 19|         Designer|       NULL|\n",
      "+-------+---+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Dropping with how=any is the default and similar to the previous line of code, let's try any=all\n",
    "df_pyspark.na.drop(how=\"all\").show() #We notice that nothing happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a09be32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------------+-----------+\n",
      "|     Name|Age|       Profession|     Income|\n",
      "+---------+---+-----------------+-----------+\n",
      "|     Alex| 25|   Data Scientist|  $120,000 |\n",
      "|   Iannis| 22|       Pro Tennis|$9,345,000 |\n",
      "|  William| 26|     Entrepreneur|$1,400,000 |\n",
      "|   Donald| 30|               IT|   $85,000 |\n",
      "|   Cedric| 31|Marketing Manager|   $90,000 |\n",
      "|   Mathew| 27|       Accountant|   $78,000 |\n",
      "|Aboubakar| 34|             NULL|   $65,000 |\n",
      "|     NULL| 19|         Designer|       NULL|\n",
      "+---------+---+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Threshold is the min number of missing values to drop the row\n",
    "df_pyspark.na.drop(how=\"any\", thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a275e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------------+-------+\n",
      "|   Name| Age|       Profession| Income|\n",
      "+-------+----+-----------------+-------+\n",
      "|   Alex|  25|   Data Scientist| 120000|\n",
      "| Iannis|  22|       Pro Tennis|9345000|\n",
      "|William|  26|     Entrepreneur|1400000|\n",
      "| Donald|  30|               IT|  85000|\n",
      "| Cedric|  31|Marketing Manager|  90000|\n",
      "| Mathew|  27|       Accountant|  78000|\n",
      "|   NULL|NULL|         Designer|  65000|\n",
      "+-------+----+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Subset of dropping\n",
    "df_pyspark.na.drop(how=\"any\", subset=['Income']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10df744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the missing values with mean of the column with Imputer, you can also set the strategy to median\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(inputCols=['Age', 'Income'],\n",
    "                 outputCols=[\"{}_imputed\".format(c) for c in ['Age', 'Income']]).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eab3e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------------+-------+-----------+--------------+\n",
      "|     Name| Age|       Profession| Income|Age_imputed|Income_imputed|\n",
      "+---------+----+-----------------+-------+-----------+--------------+\n",
      "|     Alex|  25|   Data Scientist| 120000|         25|        120000|\n",
      "|   Iannis|  22|       Pro Tennis|9345000|         22|       9345000|\n",
      "|  William|  26|     Entrepreneur|1400000|         26|       1400000|\n",
      "|   Donald|  30|               IT|  85000|         30|         85000|\n",
      "|   Cedric|  31|Marketing Manager|  90000|         31|         90000|\n",
      "|   Mathew|  27|       Accountant|  78000|         27|         78000|\n",
      "|Aboubakar|  34|             NULL|   NULL|         34|         90000|\n",
      "|     NULL|NULL|         Designer|  65000|         27|         65000|\n",
      "+---------+----+-----------------+-------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d572a1d",
   "metadata": {},
   "source": [
    "### 4. Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8df94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee30fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad8cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc012a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "550693b6",
   "metadata": {},
   "source": [
    "&copy; This notebook was inspired by the [freeCodeCamp.org Pyshark tutorial on YouTube](https://www.youtube.com/watch?v=_C8kWso4ne4). The original dataset was dropped, the one used was created by the owner of the notebook and some lines of code were intentionally changed for a better hands-on experience of the tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
